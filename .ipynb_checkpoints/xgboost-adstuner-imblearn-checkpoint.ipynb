{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost with ADSTuner for HPO\n",
    "\n",
    "* Imblearn for undersampling of negative class\n",
    "* ADSTuner for HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "import ads\n",
    "\n",
    "# to use ADSTuner\n",
    "from ads.hpo.search_cv import ADSTuner\n",
    "from ads.hpo.stopping_criterion import *\n",
    "from ads.hpo.distributions import *\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# for undersampling the negative class\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# my utils.py\n",
    "from utils import train_encoders, apply_encoders\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the ADS version\n",
    "print(ads.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global constants\n",
    "SEED = 4321\n",
    "\n",
    "# number of features (with the two indicator cols)\n",
    "N_FEATURES = 12\n",
    "\n",
    "# name of col with label\n",
    "TARGET = 'SeriousDlqin2yrs'\n",
    "\n",
    "# cols with missing values\n",
    "COL1_MISSING = 'MonthlyIncome'\n",
    "COL2_MISSING = 'NumberOfDependents'\n",
    "\n",
    "# nomi delle due colonne indicator (valgono 1 laddove il dato Ã¨ inputato)\n",
    "IND1 = 'isna_mi'\n",
    "IND2 = 'isna_nod'\n",
    "\n",
    "ind_col = [IND1, IND2]\n",
    "\n",
    "COLS_TO_DROP = ['id']\n",
    "\n",
    "# for undersampling to make the dataset more balanced\n",
    "# ratio minority samples/majority\n",
    "RATIO = 1./5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset, not undersampled\n",
    "data_full = pd.read_csv('cs-training-nonull.csv')\n",
    "\n",
    "# remove unneeded cols\n",
    "data_full = data_full.drop(COLS_TO_DROP, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['age','NumberOfTime30-59DaysPastDueNotWorse',\n",
    "               'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate',\n",
    "               'NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse',\n",
    "               'NumberOfDependents']\n",
    "num_cols = ['RevolvingUtilizationOfUnsecuredLines', 'DebtRatio', 'MonthlyIncome', ]\n",
    "\n",
    "# indicators are not touched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling and label encoding is done on data_full. After we will do resampling\n",
    "# In this way coding and scaling cover entire range of values, not only for resampled data\n",
    "\n",
    "# we don't need any scaling (it is ensambles of trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat cols treatment\n",
    "# Code categorical columns (only season, weather, year)\n",
    "\n",
    "# we don't need any pre-processing for cat columns\n",
    "\n",
    "# so for XGBoost afpret Nan treatment no other pre-processing is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estrae X: matrice features ed y, labels\n",
    "y_train_full = data_full[TARGET].values\n",
    "x_train_full = data_full.drop(TARGET, axis = 1).values\n",
    "\n",
    "assert x_train_full.shape[1] == N_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'# of samples in full dataset: {x_train_full_scaled.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the undersampling of the negative class, using IMblearn\n",
    "rus = RandomUnderSampler(sampling_strategy=RATIO, random_state=SEED)\n",
    "\n",
    "x_train, y_train = rus.fit_resample(x_train_full_scaled, y_train_full)\n",
    "\n",
    "print(f'# of samples in resampled dataset: {x_train.shape[0]}')\n",
    "\n",
    "# check ratio of classes\n",
    "print(f'# of positive samples: {np.sum(y_train)}')\n",
    "print(f'# of negative samples: {x_train.shape[0] - np.sum(y_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resampled dataset (x_train, y_train) will be used for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for the HPO session with Optuna\n",
    "FOLDS = 5\n",
    "SEED = 4321\n",
    "\n",
    "N_TRIALS = 100\n",
    "TIME_BUDGET = 7200\n",
    "STUDY_NAME = \"xgb01\"\n",
    "\n",
    "# ranges\n",
    "LR_LOW = 1e-3\n",
    "LR_HIGH = 1e-2\n",
    "DEPTH_LOW = 4\n",
    "DEPTH_HIGH = 8\n",
    "N_ITER_LIST = [600, 700, 800, 900, 1000, 1100, 1200, 1300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Here we define the strategy, the space for hyper-parameters we want to explore\n",
    "#\n",
    "params = {\n",
    "    \"n_estimators\": CategoricalDistribution(N_ITER_LIST),\n",
    "    \"learning_rate\": LogUniformDistribution(low=LR_LOW, high=LR_HIGH),\n",
    "    \"max_depth\": IntUniformDistribution(DEPTH_LOW, DEPTH_HIGH),\n",
    "}\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "\n",
    "\n",
    "# per lista scorer sorted(sklearn.metrics.SCORERS.keys())\n",
    "tuner = ADSTuner(clf, cv=FOLDS, strategy=params, scoring=\"roc_auc\", study_name=STUDY_NAME, n_jobs=6)\n",
    "\n",
    "tuner.tune(x_train, y_train, exit_criterion=[TimeBudget(TIME_BUDGET)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the status to see if completed\n",
    "print(f\"The tuner status is: {tuner.get_status()}\")\n",
    "\n",
    "print(f\"Remaining time is: {round(tuner.time_remaining, 1)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look only at completed trials, sorted with best on top. Metric chosen is in the value col.\n",
    "result_df = tuner.trials[tuner.trials[\"state\"] == \"COMPLETE\"].sort_values(\n",
    "    by=[\"value\"], ascending=False\n",
    ")\n",
    "\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tuner_results(tuner):\n",
    "\n",
    "    # to count completed\n",
    "    result_df = tuner.trials[tuner.trials[\"state\"] == \"COMPLETE\"].sort_values(\n",
    "        by=[\"value\"], ascending=False\n",
    "    )\n",
    "\n",
    "    print(\"ADSTuner session results:\")\n",
    "    print(f\"ADSTuner has launched {tuner.trials.shape[0]} trials\")\n",
    "    print(f\"ADSTuner has completed {result_df.shape[0]} trials\")\n",
    "    print()\n",
    "    print(f\"The best trial is the #: {tuner.best_index}\")\n",
    "    print(f\"Parameters for the best trial are: {tuner.best_params}\")\n",
    "    print(f\"The metric used to optimize is: {tuner.scoring_name}\")\n",
    "    print(f\"The best score is: {round(tuner.best_score, 4)}\")\n",
    "    \n",
    "show_tuner_results(tuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "clf = xgb.XGBClassifier(**tuner.best_params)\n",
    "\n",
    "# addestro e valuto su train e su validation set\n",
    "clf.fit(x_train, y_train,\n",
    "        eval_set=[(x_train, y_train)],\n",
    "        eval_metric='auc', verbose=100)\n",
    "\n",
    "print()\n",
    "\n",
    "evals_result = clf.evals_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OK, consider that the slightly higher AUC is due to the fact here we're evaluating also on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc(train_hist):\n",
    "    plt.figure(figsize=(9,6))\n",
    "    \n",
    "    plt.plot(train_hist, label='Training AUC')\n",
    "    plt.title('AUC')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('auc')\n",
    "    plt.xlabel('n_estimator')\n",
    "    plt.grid(True)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist = evals_result['validation_0']['auc']\n",
    "\n",
    "plot_auc(train_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy on full dataset\n",
    "y_pred = clf.predict(x_train_full_scaled)\n",
    "\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_train_full, predictions)\n",
    "\n",
    "print(\"Accuracy on train set: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute confusion matrix on full dataset\n",
    "tn, fp, fn, tp = confusion_matrix(y_train_full, predictions).ravel()\n",
    "\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on the TEST set (for submission to Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on test set\n",
    "orig_test = pd.read_csv('cs-test.csv')\n",
    "\n",
    "# inpute missing values, add the two indicator columns\n",
    "orig_test['isna_mi'] = 0\n",
    "orig_test.loc[orig_test[COL1_MISSING].isna(), 'isna_mi'] = 1\n",
    "orig_test.loc[orig_test[COL1_MISSING].isna(), COL1_MISSING] = MONTHLY_INC_MEDIAN\n",
    "\n",
    "orig_test['isna_nod'] = 0\n",
    "orig_test.loc[orig_test[COL2_MISSING].isna(), 'isna_nod'] = 1\n",
    "orig_test.loc[orig_test[COL2_MISSING].isna(), COL2_MISSING] = N_OF_DEP_MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_test = orig_test[ind_col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_test = orig_test.drop(ind_col, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_COL_NAME = 'Unnamed: 0'\n",
    "xorig_test = orig_test.drop(ID_COL_NAME, axis = 1)\n",
    "xorig_test = xorig_test.drop(TARGET, axis = 1)\n",
    "\n",
    "x_test = xorig_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggiungi qui lo scaling !!!\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "# riaggiunge le colonne indicatore\n",
    "x_test_scaled = np.c_[x_test_scaled, ind_test]\n",
    "\n",
    "assert x_test_scaled.shape[1] == N_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do predictions on test set (no shuffle !)\n",
    "y_pred = clf.predict_proba(x_test_scaled)\n",
    "\n",
    "# y_pred contiene le probabilitÃ \n",
    "y_pred = y_pred[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepara il csv per la submission\n",
    "result_dict = {\"Id\": orig_test[ID_COL_NAME].values,\n",
    "              'Probability': y_pred}\n",
    "\n",
    "FILE_SUB = 'submission25.csv'\n",
    "\n",
    "# build a dataframe and save to csv\n",
    "result_df = pd.DataFrame(result_dict)\n",
    "\n",
    "result_df.to_csv(FILE_SUB, index=False, float_format='%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save Modela and scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model: uso un formato semplice: pkl\n",
    "pickle.dump(clf, open(\"credit-scoring.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvo anche lo scaler\n",
    "pickle.dump(scaler, open(\"scaler.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the model\n",
    "loaded_model = pickle.load(open(\"credit-scoring.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the scaler\n",
    "loaded_scaler = pickle.load(open(\"scaler.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for online predictions\n",
    "# input are given as a numpy array, with no missing fields, but we need to add the two indicator columns\n",
    "x_input = np.array([[1,2,3,4,5,6,7,8,9,10],\n",
    "                   [1,2,3,4,5,6,7,8,9,10],\n",
    "                   [1,2,3,4,5,6,7,8,9,10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# controlli\n",
    "assert x_input.shape[1] == 10\n",
    "# check there are no null\n",
    "assert np.sum(np.isnan(x_input)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "x_input_scaled = loaded_scaler.transform(x_input)\n",
    "\n",
    "# add two columns with 0\n",
    "x_add = np.zeros((x_input.shape[0], 2))\n",
    "x_input_scaled = np.c_[x_input_scaled, x_add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = loaded_model.predict(x_input_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[TARGET].hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:generalml_p37_cpu_v1]",
   "language": "python",
   "name": "conda-env-generalml_p37_cpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
